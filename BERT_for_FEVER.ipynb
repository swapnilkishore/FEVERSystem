{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_for_FEVER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMqtAdZXSO_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install jsonlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvEmr5G4V76Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "import transformers as ppb\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-qD2OlEi04g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My \\Drive/\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQa5wMDvo0db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wc -l 'train_output_accurate.jsonl'\n",
        "!head 'train_output_accurate.jsonl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gisTWq7epZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model and tokenizer \n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkcYrt-Ic4tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define example and feature classes\n",
        "class InputExample(object):\n",
        "  def __init__(self, claim, ev, label=None):\n",
        "    self.claim = claim\n",
        "    self.ev = ev\n",
        "    self.label = label\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, claim_ids, ev_ids, label_id):\n",
        "        self.claim_ids = claim_ids\n",
        "        self.ev_ids = ev_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoa7DTHZ-hH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert training data to InputExamples\n",
        "import jsonlines\n",
        "import re\n",
        "def data_to_examples(data_file):\n",
        "  examples = []\n",
        "  data = jsonlines.open(data_file)\n",
        "  for line in data:\n",
        "    label = line['label']\n",
        "#    label = 'null'\n",
        "    claim = line['claim']\n",
        "    ev=[]\n",
        "    if line['evidence'] != 'null':\n",
        "      for item in line['evidence']:\n",
        "        #remove nonsense from sents\n",
        "        clean = re.sub('[a-zA-Z0-9]*\\t[a-zA-Z0-9]*','',item)\n",
        "        ev.append(clean)\n",
        "    else: #if evidence = null\n",
        "      ev.append('null')\n",
        "    examples.append(InputExample(claim,ev,label))\n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K533L7ArA9L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ex = data_to_examples('train_output_accurate.jsonl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rh2gwLA3aWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_ex[11].label)\n",
        "print(train_ex[11].claim)\n",
        "print(train_ex[11].ev)\n",
        "\n",
        "#label, claim, list of evidence text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ6FblZjdwPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def example_to_features(example):\n",
        "  #tokenize and add [CLS]/[SEP] tokens\n",
        "  claim_tokens = tokenizer.encode(example.claim, add_special_tokens=True)\n",
        "  ev_tokens = []\n",
        "  if example.ev ==[]:\n",
        "    ev_tokens.append([101])\n",
        "  for item in example.ev:\n",
        "    ev_tokens.append(tokenizer.encode(item,add_special_tokens=True))\n",
        "  \n",
        "  return InputFeatures(claim_tokens,ev_tokens,example.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiix5J1vOXna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_list = []\n",
        "for example in train_ex[:15000]:\n",
        "  features = example_to_features(example)\n",
        "  feat_list.append(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13cdWcmk8enq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(feat_list[11].claim_ids)\n",
        "print(feat_list[11].ev_ids)\n",
        "print(feat_list[11].label_id)\n",
        "print(len(feat_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7FOOJRF4f42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make a list of all sents (to find max_len)\n",
        "claims = [x.claim_ids for x in feat_list]\n",
        "evs = [item for x in feat_list for item in x.ev_ids]\n",
        "\n",
        "#padding\n",
        "max_len = 0\n",
        "for sent in claims+evs:\n",
        "    if len(sent) > max_len:\n",
        "        max_len = len(sent)\n",
        "print(max_len)\n",
        "\n",
        "#max_len = 327"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csFnF7UU44Mh",
        "colab_type": "code",
        "outputId": "bfbd6ae2-a3ba-43bf-f2f8-6045764b6e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "#####BOTTLENECK STEP\n",
        "#create BERT representations for each example\n",
        "BERT_examples = []\n",
        "round = 0\n",
        "for item in feat_list:\n",
        "  #padding\n",
        "  padded_claim = np.array(item.claim_ids + [0]*(max_len-len(item.claim_ids)))\n",
        "  padded_claim = np.reshape(padded_claim,(1,max_len))\n",
        "  padded_ev = np.array([sent + [0]*(max_len-len(sent)) for sent in item.ev_ids])\n",
        "  \n",
        "  #mask padded tokens\n",
        "  claim_mask = np.where(padded_claim != 0,1,0)\n",
        "  ev_mask = np.where(padded_ev != 0,1,0)\n",
        "\n",
        "  #create matrix: each row represents one sentence\n",
        "  padded_total = np.concatenate((padded_claim,padded_ev),axis=0)\n",
        "  mask_total = np.concatenate((claim_mask,ev_mask))\n",
        "\n",
        "  #convert to tensors\n",
        "  input_ids = torch.tensor(padded_total)\n",
        "  attention_mask = torch.tensor(mask_total)\n",
        "\n",
        "  #get BERT embeddings\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  #extract [CLS] embeddings and labels for input to classifier\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "  concatenated = features.flatten()\n",
        "  BERT_examples.append(list(concatenated))\n",
        "  round += 1\n",
        "  print(round)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8342\n",
            "8343\n",
            "8344\n",
            "8345\n",
            "8346\n",
            "8347\n",
            "8348\n",
            "8349\n",
            "8350\n",
            "8351\n",
            "8352\n",
            "8353\n",
            "8354\n",
            "8355\n",
            "8356\n",
            "8357\n",
            "8358\n",
            "8359\n",
            "8360\n",
            "8361\n",
            "8362\n",
            "8363\n",
            "8364\n",
            "8365\n",
            "8366\n",
            "8367\n",
            "8368\n",
            "8369\n",
            "8370\n",
            "8371\n",
            "8372\n",
            "8373\n",
            "8374\n",
            "8375\n",
            "8376\n",
            "8377\n",
            "8378\n",
            "8379\n",
            "8380\n",
            "8381\n",
            "8382\n",
            "8383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I9bb7xXtp4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(padded_claim)\n",
        "print(padded_ev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-1kdB8geJ6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(BERT_examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWRyuRRdi_cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pad each example for uniform input size\n",
        "max_len = 0\n",
        "for example in BERT_examples:\n",
        "  if len(example) > max_len:\n",
        "    max_len = len(example)\n",
        "\n",
        "padded_BERT = []\n",
        "for example in BERT_examples:\n",
        "  padded_BERT.append(example + [0]*(max_len-len(example)))\n",
        "\n",
        "print(max_len)\n",
        "print(len(padded_BERT[40]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VlhWm11JwqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [item.label_id for item in feat_list[:len(BERT_examples)]]\n",
        "train_inputs = np.array([np.array(ex) for ex in padded_BERT])\n",
        "print(train_inputs.shape)\n",
        "\n",
        "#feat_list[:10000] model: max_len = 109824\n",
        "#feat_list[:15000] model: max_len = 109824"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bCfKt9cXC2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train classifier on embeddings/labels\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(train_inputs, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnTFS4H46jEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# save the model to drive\n",
        "filename = 'new_trained_model_15k.sav'\n",
        "pickle.dump(log_reg, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfsG7xGg61NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the trained model from drive\n",
        "loaded_model = pickle.load(open('new_trained_model_15k.sav', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cADpAPNsXDXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test classifier accuracy\n",
        "loaded_model.score(train_inputs, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyC1O4NmMaze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#10k train accuracy = 0.9306\n",
        "#15k train accuracy = 0.9412"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}